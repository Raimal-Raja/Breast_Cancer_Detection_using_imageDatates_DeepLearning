{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f17d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Breast Cancer Detection - AutoML Training\n",
    "# Dataset: Breast Histopathology Images\n",
    "# Target Accuracy: 90%+\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATASET PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset path\n",
    "DATASET_PATH = '/kaggle/input/breast-histopathology-images'\n",
    "BASE_DIR = '/kaggle/working'\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "for root, dirs, files in os.walk(DATASET_PATH):\n",
    "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    if level < 2:\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:3]:\n",
    "            print(f'{subindent}{file}')\n",
    "        if len(files) > 3:\n",
    "            print(f'{subindent}... and {len(files)-3} more files')\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: LOAD AND ORGANIZE DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_dataset_info(dataset_path):\n",
    "    \"\"\"Load and organize dataset information\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    patient_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    \n",
    "    print(f\"\\nFound {len(patient_dirs)} patient directories\")\n",
    "    \n",
    "    for patient_id in tqdm(patient_dirs, desc=\"Loading dataset\"):\n",
    "        patient_path = os.path.join(dataset_path, patient_id)\n",
    "        \n",
    "        for img_class in ['0', '1']:\n",
    "            class_path = os.path.join(patient_path, img_class)\n",
    "            if os.path.exists(class_path):\n",
    "                for img_file in os.listdir(class_path):\n",
    "                    if img_file.endswith('.png'):\n",
    "                        image_paths.append(os.path.join(class_path, img_file))\n",
    "                        labels.append(int(img_class))\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "print(\"Loading dataset information...\")\n",
    "image_paths, labels = load_dataset_info(DATASET_PATH)\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_paths)}\")\n",
    "print(f\"Positive samples (IDC): {sum(labels)}\")\n",
    "print(f\"Negative samples (No IDC): {len(labels) - sum(labels)}\")\n",
    "print(f\"Class balance: {sum(labels)/len(labels)*100:.2f}% positive\")\n",
    "\n",
    "# Create DataFrame with string labels for Keras\n",
    "df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': [str(label) for label in labels]  # Convert to strings\n",
    "})\n",
    "\n",
    "# Save dataset info\n",
    "df.to_csv(os.path.join(BASE_DIR, 'dataset_info.csv'), index=False)\n",
    "print(f\"\\nDataset info saved to dataset_info.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA SPLITTING\n",
    "# ============================================================================\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DATA GENERATORS WITH AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "IMG_SIZE = 96  # Scaled up from 50x50 for better feature extraction\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Training data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation and test data (only rescaling)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    classes=['0', '1'],  # Explicit class mapping\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    classes=['0', '1'],  # Explicit class mapping\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    classes=['0', '1'],  # Explicit class mapping\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: BUILD AUTOML MODEL (Transfer Learning with EfficientNetB0)\n",
    "# ============================================================================\n",
    "\n",
    "def build_model(img_size=96, use_pretrained=True):\n",
    "    \"\"\"Build EfficientNetB0-based model for AutoML approach\"\"\"\n",
    "    \n",
    "    if use_pretrained:\n",
    "        try:\n",
    "            # Try to load pre-trained EfficientNetB0\n",
    "            print(\"Attempting to download pre-trained EfficientNetB0 weights...\")\n",
    "            base_model = EfficientNetB0(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                input_shape=(img_size, img_size, 3)\n",
    "            )\n",
    "            print(\"Pre-trained weights loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download pre-trained weights: {e}\")\n",
    "            print(\"Falling back to custom CNN architecture...\")\n",
    "            use_pretrained = False\n",
    "    \n",
    "    if not use_pretrained:\n",
    "        # Build custom CNN from scratch (no internet required)\n",
    "        base_model = models.Sequential([\n",
    "            # First Conv Block\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                         input_shape=(img_size, img_size, 3)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Second Conv Block\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Fourth Conv Block\n",
    "            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "        ], name='custom_cnn_base')\n",
    "    \n",
    "    # Freeze base model initially (if using pre-trained)\n",
    "    if use_pretrained:\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "print(\"\\nBuilding AutoML model...\")\n",
    "model, base_model = build_model(IMG_SIZE)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc'), \n",
    "             keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: CALLBACKS\n",
    "# ============================================================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: TRAIN MODEL (PHASE 1 - Frozen Base)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: Training with frozen base model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: FINE-TUNING (PHASE 2 - Unfrozen Base)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: Fine-tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if base model is trainable (only for pre-trained models)\n",
    "if hasattr(base_model, 'layers') and len(base_model.layers) > 100:\n",
    "    # Unfreeze base model (for transfer learning)\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze early layers, unfreeze later layers\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(\"Fine-tuning with unfrozen base model layers\")\n",
    "else:\n",
    "    # Custom CNN - all layers already trainable\n",
    "    print(\"Continuing training with all layers trainable\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc'),\n",
    "             keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: EVALUATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "model = keras.models.load_model(os.path.join(MODEL_DIR, 'best_model.h5'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc, test_precision, test_recall = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1-Score: {2*(test_precision*test_recall)/(test_precision+test_recall):.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "test_generator.reset()\n",
    "y_pred_proba = model.predict(test_generator)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "y_true = test_df['label'].astype(int).values  # Convert string back to int\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['No IDC (0)', 'IDC (1)']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No IDC', 'IDC'],\n",
    "            yticklabels=['No IDC', 'IDC'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(BASE_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(os.path.join(BASE_DIR, 'roc_curve.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Training history plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Combine histories\n",
    "all_history = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "}\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(all_history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(all_history['val_accuracy'], label='Validation')\n",
    "axes[0, 0].set_title('Model Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(all_history['loss'], label='Train')\n",
    "axes[0, 1].plot(all_history['val_loss'], label='Validation')\n",
    "axes[0, 1].set_title('Model Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Sample predictions\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: SAVE MODEL FOR DEPLOYMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save as .h5 format\n",
    "model.save(os.path.join(MODEL_DIR, 'breast_cancer_model.h5'))\n",
    "print(f\"Model saved as: {os.path.join(MODEL_DIR, 'breast_cancer_model.h5')}\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'img_size': IMG_SIZE,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'class_names': ['No IDC (Negative)', 'IDC Positive']\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model_config.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "\n",
    "print(f\"Model config saved as: {os.path.join(MODEL_DIR, 'model_config.pkl')}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'total_images': len(df),\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'test_f1': float(2*(test_precision*test_recall)/(test_precision+test_recall))\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results_summary])\n",
    "results_df.to_csv(os.path.join(BASE_DIR, 'model_results.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"\\nFiles to download:\")\n",
    "print(f\"1. {os.path.join(MODEL_DIR, 'breast_cancer_model.h5')}\")\n",
    "print(f\"2. {os.path.join(MODEL_DIR, 'model_config.pkl')}\")\n",
    "print(f\"3. {os.path.join(BASE_DIR, 'model_results.csv')}\")\n",
    "print(f\"4. {os.path.join(BASE_DIR, 'confusion_matrix.png')}\")\n",
    "print(f\"5. {os.path.join(BASE_DIR, 'roc_curve.png')}\")\n",
    "print(f\"6. {os.path.join(BASE_DIR, 'training_history.png')}\")\n",
    "print(\"\\nDownload these files and place them in your Flask app directory!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
