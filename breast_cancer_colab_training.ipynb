{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17230393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BREAST CANCER DETECTION - GOOGLE COLAB TRAINING (TENSORFLOW 2.19+ FIX)\n",
    "# Dataset: Breast Histopathology Images (277,524+ images)\n",
    "# Target Accuracy: 90%+\n",
    "# FIX: Compatible with TensorFlow 2.19.0+ - Resolves class_weight with generator issue\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: MOUNT GOOGLE DRIVE AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BREAST CANCER DETECTION - GOOGLE COLAB (TF 2.19+ COMPATIBLE)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: SETUP KAGGLE API AND DOWNLOAD DATASET\n",
    "# ============================================================================\n",
    "\n",
    "# Create Kaggle directory\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Create kaggle.json with your credentials\n",
    "kaggle_credentials = {\n",
    "    \"username\": \"professorraimal\",\n",
    "    \"key\": \"KGAT_24f6310a3bc8ef58c1c084f178820236\"\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_credentials, f)\n",
    "\n",
    "# Set permissions\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n✓ Kaggle API configured successfully!\")\n",
    "\n",
    "# Download dataset\n",
    "print(\"\\nDownloading Breast Histopathology Images dataset...\")\n",
    "print(\"This may take 5-10 minutes depending on your internet speed...\")\n",
    "\n",
    "!kaggle datasets download -d paultimothymooney/breast-histopathology-images\n",
    "\n",
    "print(\"\\n✓ Dataset downloaded!\")\n",
    "\n",
    "# Unzip dataset\n",
    "print(\"\\nExtracting dataset...\")\n",
    "!unzip -q breast-histopathology-images.zip -d /content/dataset\n",
    "\n",
    "print(\"✓ Dataset extracted successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CREATE GOOGLE DRIVE FOLDER STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "# Create project folders in Google Drive\n",
    "DRIVE_BASE = '/content/drive/MyDrive/BreastCancerDetection'\n",
    "MODEL_DIR = os.path.join(DRIVE_BASE, 'models')\n",
    "RESULTS_DIR = os.path.join(DRIVE_BASE, 'results')\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Created folder structure in Google Drive:\")\n",
    "print(f\"  - {DRIVE_BASE}\")\n",
    "print(f\"  - {MODEL_DIR}\")\n",
    "print(f\"  - {RESULTS_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: EXPLORE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "DATASET_PATH = '/content/dataset'\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPLORING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def count_images(dataset_path):\n",
    "    \"\"\"Count total images and class distribution\"\"\"\n",
    "    total_images = 0\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                total_images += 1\n",
    "                if '/1/' in root:\n",
    "                    positive_count += 1\n",
    "                elif '/0/' in root:\n",
    "                    negative_count += 1\n",
    "\n",
    "    return total_images, positive_count, negative_count\n",
    "\n",
    "total, positive, negative = count_images(DATASET_PATH)\n",
    "print(f\"\\nTotal Images: {total:,}\")\n",
    "print(f\"IDC Positive (Class 1): {positive:,} ({positive/total*100:.2f}%)\")\n",
    "print(f\"No IDC (Class 0): {negative:,} ({negative/total*100:.2f}%)\")\n",
    "print(f\"\\n✓ Dataset has {total:,} images (Requirement: 20,000+)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: LOAD AND ORGANIZE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def load_dataset_info(dataset_path, sample_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Load dataset information\n",
    "    sample_fraction: 1.0 for full dataset, 0.3 for 30% sample\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Find base path\n",
    "    for item in os.listdir(dataset_path):\n",
    "        item_path = os.path.join(dataset_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            if 'IDC' in item or len(os.listdir(item_path)) > 100:\n",
    "                dataset_path = item_path\n",
    "                break\n",
    "\n",
    "    patient_dirs = []\n",
    "    for item in os.listdir(dataset_path):\n",
    "        item_path = os.path.join(dataset_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            patient_dirs.append(item_path)\n",
    "\n",
    "    print(f\"Found {len(patient_dirs)} patient directories\")\n",
    "\n",
    "    # Sample if needed\n",
    "    if sample_fraction < 1.0:\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        n_sample = int(len(patient_dirs) * sample_fraction)\n",
    "        patient_dirs = random.sample(patient_dirs, n_sample)\n",
    "        print(f\"Sampling {sample_fraction*100}%: {len(patient_dirs)} directories\")\n",
    "\n",
    "    # Load image paths and labels\n",
    "    for patient_path in tqdm(patient_dirs, desc=\"Loading images\"):\n",
    "        for img_class in ['0', '1']:\n",
    "            class_path = os.path.join(patient_path, img_class)\n",
    "            if os.path.exists(class_path):\n",
    "                for img_file in os.listdir(class_path):\n",
    "                    if img_file.endswith('.png'):\n",
    "                        image_paths.append(os.path.join(class_path, img_file))\n",
    "                        labels.append(int(img_class))\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "# Load full dataset for 90%+ accuracy\n",
    "print(\"Loading dataset information...\")\n",
    "image_paths, labels = load_dataset_info(DATASET_PATH, sample_fraction=1.0)\n",
    "\n",
    "print(f\"\\n✓ Loaded Images: {len(image_paths):,}\")\n",
    "print(f\"  Positive samples (IDC): {sum(labels):,}\")\n",
    "print(f\"  Negative samples (No IDC): {len(labels) - sum(labels):,}\")\n",
    "print(f\"  Class balance: {sum(labels)/len(labels)*100:.2f}% positive\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SPLIT DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Training: {len(train_df):,} images\")\n",
    "print(f\"  Validation: {len(val_df):,} images\")\n",
    "print(f\"  Test: {len(test_df):,} images\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: SETUP DATA GENERATORS (WITH CLASS WEIGHT FIX)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURING DATA AUGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Training augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation/Test: only rescaling\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def create_generator(df, datagen, batch_size, shuffle=True, class_weights=None):\n",
    "    \"\"\"\n",
    "    Create data generator with optional class weights\n",
    "    FIX: class_weights applied inside generator (TF 2.19+ compatible)\n",
    "    When class_weights are provided, yields (images, labels, sample_weights)\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        while True:\n",
    "            if shuffle:\n",
    "                df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "            else:\n",
    "                df_shuffled = df\n",
    "\n",
    "            for i in range(0, len(df_shuffled), batch_size):\n",
    "                batch_df = df_shuffled.iloc[i:i+batch_size]\n",
    "\n",
    "                images = []\n",
    "                labels = []\n",
    "\n",
    "                for _, row in batch_df.iterrows():\n",
    "                    img = cv2.imread(row['image_path'])\n",
    "                    if img is not None:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                        images.append(img)\n",
    "                        labels.append(row['label'])\n",
    "\n",
    "                if len(images) > 0:\n",
    "                    images = np.array(images, dtype=np.float32)\n",
    "                    labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "                    # Apply augmentation\n",
    "                    for j in range(len(images)):\n",
    "                        images[j] = datagen.random_transform(images[j])\n",
    "                    images = datagen.standardize(images)\n",
    "\n",
    "                    # FIX: Apply sample weights if provided (TF 2.19+ compatible)\n",
    "                    if class_weights is not None:\n",
    "                        sample_weights = np.array([class_weights[int(label)] for label in labels])\n",
    "                        yield images, labels, sample_weights\n",
    "                    else:\n",
    "                        yield images, labels\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(f\"\\nClass weights: {class_weights_dict}\")\n",
    "print(\"✓ Class weights will be applied inside generator (TF 2.19+ compatible)\")\n",
    "\n",
    "# Create generators WITH class weights for training\n",
    "train_gen = create_generator(train_df, train_datagen, BATCH_SIZE, shuffle=True, class_weights=class_weights_dict)\n",
    "val_gen = create_generator(val_df, val_test_datagen, BATCH_SIZE, shuffle=False, class_weights=None)\n",
    "test_gen = create_generator(test_df, val_test_datagen, BATCH_SIZE, shuffle=False, class_weights=None)\n",
    "\n",
    "# Calculate steps\n",
    "train_steps = len(train_df) // BATCH_SIZE\n",
    "val_steps = len(val_df) // BATCH_SIZE\n",
    "test_steps = len(test_df) // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nData Generators Created:\")\n",
    "print(f\"  Training steps per epoch: {train_steps}\")\n",
    "print(f\"  Validation steps per epoch: {val_steps}\")\n",
    "print(f\"  Test steps: {test_steps}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: BUILD MODEL (TENSORFLOW 2.19+ COMPATIBLE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING MODEL (TF 2.19+ COMPATIBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_model(model_type='efficientnet'):\n",
    "    \"\"\"\n",
    "    Build model with TensorFlow 2.19+ compatibility fix\n",
    "    FIX: Removed include_preprocessing parameter - not available in TF 2.19\n",
    "    \"\"\"\n",
    "    if model_type == 'efficientnet':\n",
    "        # FIX: Don't use include_preprocessing - it doesn't exist in TF 2.19\n",
    "        base_model = EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "        )\n",
    "        print(\"Using EfficientNetB0 as base model\")\n",
    "    else:\n",
    "        base_model = MobileNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "        )\n",
    "        print(\"Using MobileNetV2 as base model\")\n",
    "\n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Build model using Functional API for better compatibility\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Try EfficientNetB0 first, fallback to MobileNetV2 if needed\n",
    "try:\n",
    "    model, base_model = build_model('efficientnet')\n",
    "    model_type = 'efficientnet'\n",
    "except Exception as e:\n",
    "    print(f\"⚠ EfficientNet failed: {e}\")\n",
    "    print(\"Falling back to MobileNetV2...\")\n",
    "    model, base_model = build_model('mobilenet')\n",
    "    model_type = 'mobilenet'\n",
    "\n",
    "print(f\"\\n✓ Model built successfully ({model_type.upper()})\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: COMPILE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: SETUP CALLBACKS\n",
    "# ============================================================================\n",
    "\n",
    "# Save best model during training - use .keras format\n",
    "checkpoint_path = os.path.join(MODEL_DIR, 'best_model.keras')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback, early_stopping, reduce_lr]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: TRAIN MODEL (TWO-PHASE TRAINING) - FIXED FOR TF 2.19+\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: TRAINING WITH FROZEN BASE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "epochs_phase1 = 6\n",
    "\n",
    "# FIX: Remove class_weight parameter - weights are in generator now\n",
    "history_phase1 = model.fit(\n",
    "    train_gen(),\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_gen(),\n",
    "    validation_steps=val_steps,\n",
    "    epochs=epochs_phase1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Unfreeze last layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Unfrozen {sum([layer.trainable for layer in base_model.layers])} layers\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "epochs_phase2 = 5\n",
    "\n",
    "# FIX: Remove class_weight parameter - weights are in generator now\n",
    "history_phase2 = model.fit(\n",
    "    train_gen(),\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_gen(),\n",
    "    validation_steps=val_steps,\n",
    "    epochs=epochs_phase2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 12: EVALUATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "print(f\"\\nLoading best model from: {checkpoint_path}\")\n",
    "model = keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(\n",
    "    test_gen(),\n",
    "    steps=test_steps,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Get predictions for detailed metrics\n",
    "print(\"\\nGenerating predictions for detailed metrics...\")\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(test_steps):\n",
    "    batch_data = next(test_gen())\n",
    "    X_batch = batch_data[0]\n",
    "    y_batch = batch_data[1]\n",
    "    y_pred_proba.extend(model.predict(X_batch, verbose=0).flatten())\n",
    "    y_true.extend(y_batch)\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "y_true = np.array(y_true)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['No IDC', 'IDC Positive']))\n",
    "\n",
    "# Additional metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "test_precision = precision_score(y_true, y_pred)\n",
    "test_recall = recall_score(y_true, y_pred)\n",
    "test_f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 13: VISUALIZE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "           xticklabels=['No IDC', 'IDC Positive'],\n",
    "           yticklabels=['No IDC', 'IDC Positive'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'confusion_matrix.png'),\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "        label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'roc_curve.png'),\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "all_history = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "}\n",
    "\n",
    "axes[0].plot(all_history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(all_history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(all_history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(all_history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'training_history.png'),\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 14: SAVE MODEL FOR DEPLOYMENT (TF 2.19+ COMPATIBLE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL FOR DEPLOYMENT (TF 2.19+ COMPATIBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save in both .keras (native) and .h5 (compatibility) formats\n",
    "keras_model_path = os.path.join(MODEL_DIR, 'breast_cancer_model.keras')\n",
    "h5_model_path = os.path.join(MODEL_DIR, 'breast_cancer_model.h5')\n",
    "\n",
    "# Save in native Keras format (recommended for TF 2.19+)\n",
    "print(\"Saving model in .keras format...\")\n",
    "model.save(keras_model_path)\n",
    "print(f\"✓ Model saved (Keras format): {keras_model_path}\")\n",
    "\n",
    "# Save in H5 format for backward compatibility\n",
    "print(\"\\nSaving model in .h5 format...\")\n",
    "try:\n",
    "    model.save(h5_model_path, save_format='h5')\n",
    "    print(f\"✓ Model saved (H5 format): {h5_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Warning: H5 save failed: {e}\")\n",
    "    print(\"  Using .keras format only (recommended for TF 2.19+)\")\n",
    "\n",
    "# Verify file sizes\n",
    "print(\"\\nVerifying saved files...\")\n",
    "if os.path.exists(keras_model_path):\n",
    "    size_mb = os.path.getsize(keras_model_path) / (1024 * 1024)\n",
    "    print(f\"✓ .keras file: {size_mb:.2f} MB\")\n",
    "if os.path.exists(h5_model_path):\n",
    "    size_mb = os.path.getsize(h5_model_path) / (1024 * 1024)\n",
    "    print(f\"✓ .h5 file: {size_mb:.2f} MB\")\n",
    "\n",
    "# Save configuration\n",
    "model_config = {\n",
    "    'img_size': IMG_SIZE,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'test_f1': float(test_f1),\n",
    "    'class_names': ['No IDC (Negative)', 'IDC Positive'],\n",
    "    'model_type': model_type,\n",
    "    'tensorflow_version': tf.__version__\n",
    "}\n",
    "\n",
    "config_path = os.path.join(MODEL_DIR, 'model_config.pkl')\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "print(f\"✓ Config saved: {config_path}\")\n",
    "\n",
    "# Verify config file\n",
    "if os.path.exists(config_path):\n",
    "    size_kb = os.path.getsize(config_path) / 1024\n",
    "    print(f\"✓ Config file: {size_kb:.2f} KB\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'total_images': len(df),\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'test_f1': float(test_f1),\n",
    "    'model_type': model_type,\n",
    "    'tensorflow_version': tf.__version__\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(os.path.join(RESULTS_DIR, 'model_results.csv'), index=False)\n",
    "print(f\"✓ Results saved: {os.path.join(RESULTS_DIR, 'model_results.csv')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Model Type: {model_type.upper()}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FILES SAVED TO GOOGLE DRIVE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nModel Files:\")\n",
    "print(f\"  1. {keras_model_path}\")\n",
    "if os.path.exists(h5_model_path):\n",
    "    print(f\"  2. {h5_model_path}\")\n",
    "print(f\"  3. {config_path}\")\n",
    "print(f\"\\nVisualization Files:\")\n",
    "print(f\"  4. {os.path.join(RESULTS_DIR, 'model_results.csv')}\")\n",
    "print(f\"  5. {os.path.join(RESULTS_DIR, 'confusion_matrix.png')}\")\n",
    "print(f\"  6. {os.path.join(RESULTS_DIR, 'roc_curve.png')}\")\n",
    "print(f\"  7. {os.path.join(RESULTS_DIR, 'training_history.png')}\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"NEXT STEPS:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"1. Go to Google Drive: MyDrive/BreastCancerDetection/models/\")\n",
    "print(\"2. Download:\")\n",
    "print(\"   ✓ breast_cancer_model.keras (RECOMMENDED, ~50MB)\")\n",
    "print(\"   ✓ model_config.pkl (~1KB)\")\n",
    "print(\"   (Optional: breast_cancer_model.h5 if available)\")\n",
    "print(\"3. Place them in your Flask app's 'models/' folder\")\n",
    "print(\"4. Run the Flask application locally\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Display Google Drive link\n",
    "print(f\"\\n✓ Access your files here:\")\n",
    "print(f\"https://drive.google.com/drive/folders/MyDrive/BreastCancerDetection\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUCCESS! Model training completed successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1642fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878eb13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
